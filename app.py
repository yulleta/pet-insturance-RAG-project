# app.py

import streamlit as st
import openai
import pinecone
import json
import pandas as pd
import re
from typing import List, Dict, Tuple
from pinecone import Pinecone, ServerlessSpec
import os
from openai import OpenAI
import json

# â”€â”€â”€ í™˜ê²½ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="ë³´í—˜ ë¹„êµ RAG", layout="wide")

# OpenAI, Pinecone API í‚¤
# openai.api_key = OPENAI_API

# pc = pinecone.Pinecone(api_key=PINECONE_API)
index = pc.Index("pet-insurance-chunks")

EMBED_MODEL = "text-embedding-3-large"
# client = OpenAI(api_key=OPENAI_API)


# â”€â”€â”€ ìœ í‹¸ í•¨ìˆ˜ë“¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
W_VEC, W_CAT, W_KEY = 0.6, 0.2, 0.2

def classify_question(question: str) -> str:
    prompt = (
        f"ì§ˆë¬¸: \"{question}\"\n"
        "ìœ„ ì§ˆë¬¸ì´ ë‹¤ìŒ ì¤‘ ì–´ëŠ ìœ í˜•ì¸ì§€ í•˜ë‚˜ë§Œ ê³¨ë¼ì„œ ì •í™•í•˜ê²Œ í•œ ë‹¨ì–´ë¡œ ì•Œë ¤ì£¼ì„¸ìš”:\n"
        "ë³´ì¥_ë²”ìœ„\në³´ì¥_ì¡°ê±´\në³´ì¥_ì ˆì°¨\n"
    )
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )
    return response.choices[0].message.content.strip()

def compute_keyword_score(keywords: List[str], question: str) -> float:
    tokens = re.findall(r"[ê°€-í£a-zA-Z0-9]+", question.lower())
    matched = sum(1 for kw in keywords if any(kw.lower() in token for token in tokens))
    return matched / len(keywords) if keywords else 0.0

def embed_text(text: str) -> List[float]:
    """í…ìŠ¤íŠ¸ë¥¼ OpenAI ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜"""
    resp = openai.embeddings.create(model=EMBED_MODEL, input=[text])
    return resp.data[0].embedding

def retrieve_top_chunks_by_company(
    question: str,
    top_k_per_company: int = 10,
    search_k: int = 100
) -> Tuple[Dict[str, List[Dict]], str]:
    q_type = classify_question(question)
    
    print(q_type)
    
    q_vec = embed_text(question)

    resp = index.query(
        vector=q_vec,
        top_k=search_k,
        include_metadata=True
    )

    company_buckets: Dict[str, List[Tuple[float, Dict]]] = {}

    for match in resp.matches:
        md = match.metadata
        company = md.get("company", "unknown")
        vec_score = match.score
        cat_score = md.get(q_type, 0.0)
        key_score = compute_keyword_score(md["top_keywords"], question)

        combined = W_VEC * vec_score + W_CAT * cat_score + W_KEY * key_score
        company_buckets.setdefault(company, []).append((combined, md))

    top_chunks_by_company: Dict[str, List[Dict]] = {}

    for comp, items in company_buckets.items():
        items.sort(key=lambda x: x[0], reverse=True)
        top_chunks_by_company[comp] = [md for _, md in items[:top_k_per_company]]

    return top_chunks_by_company, q_type

def generate_comparative_answer(question: str) -> str:
    company_chunks, q_type = retrieve_top_chunks_by_company(question)

    # ğŸ”¹ íšŒì‚¬ë³„ ë¬¸ì„œ ìš”ì•½ ì •ë¦¬
    company_sections = []
    for company, chunks in company_chunks.items():
        summaries = "\n".join([f"- {md['raw_sentences']}" for md in chunks])
        section = f"**{company}**\n{summaries}"
        company_sections.append(section)

    context = "\n\n".join(company_sections)

    # ğŸ”¹ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = (
        f"[ì§ˆë¬¸]\n{question}\n\n"
        f"[ì•½ê´€ ìš”ì•½ - ì§ˆì˜ìœ í˜•: {q_type}]\n{context}\n\n"
        "ìœ„ ì•½ê´€ ë‚´ìš©ë“¤ì„ ì°¸ê³ í•˜ì—¬ ê° ë³´í—˜ì‚¬(ì‚¼ì„±í™”ì¬, ë©”ë¦¬ì¸ í™”ì¬, í•œí™”ë³´í—˜ 3ì‚¬)ì˜ "
        "ë³´ì¥ ì—¬ë¶€ë‚˜ ì¡°ê±´ì„ ì •ë¦¬í•˜ê³  ë¹„êµí•´ì£¼ì„¸ìš”. "
        "íšŒì‚¬ì˜ ì´ë¦„ì„ ëª…í™•íˆ êµ¬ë¶„í•´ì„œ ìš”ì•½í•´ ì£¼ì„¸ìš”."
        "ë‹µë³€ì€ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ í‘œë¡œ ë‚˜íƒ€ë‚´ì–´ ë¹„êµí•´ì£¼ì„¸ìš”. ê° íšŒì‚¬ 3ì‚¬ê°€ headerì´ê³  rowì˜ ê° í–‰ ë‚´ìš©ì€ ì•Œì•„ì„œ ì˜ ì±„ì›Œì£¼ì„¸ìš”."
        "í‘œ í•˜ë‹¨ì—ëŠ” ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì¤„ê¸€ë¡œ ê° íšŒì‚¬ë¥¼ ì´ ì •ë¦¬ ë¹„êµí•˜ì—¬ ê°€ì¥ ì¢‹ì€ ë³´í—˜ì‚¬ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”."
    )

    # ğŸ”¹ GPT í˜¸ì¶œ (í•œ ë²ˆë§Œ)
    response = client.chat.completions.create(
        model="gpt-4.1-nano",
        messages=[{"role": "user", "content": prompt}],
    )

    return response.choices[0].message.content.strip()

# âœ… JSON í‚¤ì›Œë“œ ë¡œë“œ
@st.cache_data
def load_keywords(json_path='cluster_keywords.json'):
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # í´ëŸ¬ìŠ¤í„° êµ¬ë¶„ ì—†ì´ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ í•©ì¹˜ê¸°
    keyword_set = set()
    for keyword_list in data.values():
        if isinstance(keyword_list, list):
            for kw_group in keyword_list:
                if isinstance(kw_group, str):
                    for kw in kw_group.split(','):
                        keyword_set.add(kw.strip())
    return sorted(keyword_set)

def get_recommendations(input_text, keyword_list, max_suggestions=10):
    if not input_text:
        return []

    input_chars = set(input_text.lower())

    scored = []
    for kw in keyword_list:
        overlap = len(input_chars & set(kw.lower()))
        if overlap > 2:
            scored.append((kw, overlap))

    # ê²¹ì¹˜ëŠ” ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    scored.sort(key=lambda x: -x[1])

    return [kw for kw, _ in scored[:max_suggestions]]



# â”€â”€â”€ Streamlit UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown("## ğŸ“Œ ë°˜ë ¤ë™ë¬¼ ë³´í—˜ ë¹„êµ ì§ˆë¬¸ Q&A")

# 1. ì§ˆë¬¸ ì…ë ¥
question = st.text_input(
    "ë³´í—˜ ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:",
    value="ê³ ë ¹ ë°˜ë ¤ê²¬(ì˜ˆ: 10ì‚´ ì´ìƒ)ì€ ë³´í—˜ ê°€ì…ì´ ë¶ˆê°€ëŠ¥í•œê°€ìš”?"
)


# 2. í‚¤ì›Œë“œ ì¶”ì²œ (ë™ì‹œì— ì…ë ¥ëœ ì§ˆë¬¸ ê¸°ì¤€)
keywords = load_keywords()
# ğŸ” ì¶”ì²œ í‚¤ì›Œë“œ (ì‹¤ì‹œê°„ ë°˜ì‘)
matched_keywords = get_recommendations(question, keywords)
if matched_keywords:
    st.markdown("#### ğŸ” ì¶”ì²œ í‚¤ì›Œë“œ")
    st.markdown(", ".join(f"`{kw}`" for kw in matched_keywords))

# 1. ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ì¶”ì²œ í‚¤ì›Œë“œ ë¬¸ìì—´
keyword_str = ", ".join(matched_keywords)

# 2. ì§ˆë¬¸ + í‚¤ì›Œë“œ ë¬¸ìì—´ ë¬¶ì–´ì„œ GPTì— ì „ë‹¬
full_query = f"[ì§ˆë¬¸]\n{question}\n\n[ê´€ë ¨ í‚¤ì›Œë“œ]\n{keyword_str}"

# 3. GPT í˜¸ì¶œ
if st.button("ë¹„êµí•˜ê¸°"):
    if not question.strip():
        st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        with st.spinner("ê²€ìƒ‰ ë° ë¹„êµ ìƒì„± ì¤‘â€¦"):
            try:
                answer = generate_comparative_answer(full_query)
                st.markdown("### ğŸ§¾ ë¹„êµ ê²°ê³¼")
                st.markdown(answer)
            except Exception as e:
                st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
# python -m streamlit run app.py